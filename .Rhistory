third <- read.csv2(file="C:/Users/55131065/Desktop/code_combined/crime_tree_22_01_21.csv", sep=",", head=TRUE)
third <- third[!duplicated(third$HO_5_Digit_Code),]
nrow(third)
third <- third %>%
select(HO_5_Digit_Code, HO_Crime_Category)%>%
rename(Code = HO_5_Digit_Code, Home3 = HO_Crime_Category)
dat <- first %>%
left_join(third)
dat
third <- read.csv2(file="C:/Users/55131065/Desktop/code_combined/crime_tree_22_01_21.csv", sep=",", head=TRUE)
third <- third[!duplicated(third$HO_5_Digit_Code),]
which(!third$HO_5_Digit_Code %in% first$Code)
third[which(!third$HO_5_Digit_Code %in% first$Code),]
setwd("C:/R/Github/JGIS_Policing_COVID-19/")
regions <- c("North West", "North East","Yorkshire and the Humber",
"West Midlands","East Midlands","Eastern",
"Wales", "South West","South East")
t1 <- Sys.time()
op <- par(mfrow = c(2,2),
oma = c(5,4,0,0) + 0.5,
mar = c(0,0,4,4) + 0.5)
#all_UK_bing_combined <- NULL
all_Regional_afinn_combined <- NULL
all_pvalueS_1 <- NULL
all_pvalueS_2 <- NULL
all_pvalueS_3 <- NULL
all_p_signs_1 <- matrix(0, 42, 3)
all_p_signs_2 <- matrix(0, 42, 3)
all_p_signs_3 <- matrix(0, 42, 3)
init <- 0
#ge
library(opitools)
i=1
Pf_names_regions
getwd()
Pf_names_regions <- read.table(file="/data/Regions.csv", sep=",", head=TRUE)
Pf_names_regions <- read.table(file="/data/Regions.csv", sep=",", head=TRUE)
setwd(WORKING_DIR)
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'
setwd(WORKING_DIR)
Pf_names_regions <- read.table(file="/data/Regions.csv", sep=",", head=TRUE)
Pf_names_regions <- read.table(file="C:/R/Github/JGIS_Policing_COVID-19/data/Regions.csv", sep=",", head=TRUE)
WORKING_DIR
setwd(WORKING_DIR)
Pf_names_regions <- read.table(file="/data/Regions.csv", sep=",", head=TRUE)
read.csv(file=paste(WORKING_DIR, "/data/PoliceForce_location.csv", sep=""), sep=",", head=TRUE)
Pf_names_regions <- read.table(file=paste(WORKING_DIR, "/data/Regions.csv", sep="", sep=",", head=TRUE)
Pf_names_regions <- read.table(file=paste(WORKING_DIR, "/data/Regions.csv", sep=""), sep=",", head=TRUE)
data_sample1 <- readRDS(file=paste(WORKING_DIR, "/data/tweets_containing_only_policing_28_69.rds", sep=""))
paste(WORKING_DIR, "/data/tweets_containing_only_policing_28_69.rds", sep="")
data_sample1 <- readRDS(file=paste(WORKING_DIR, "/data/tweets_containing_only_policing_28_69.rds", sep=""))
data_sample1 <- readRDS(file=paste(WORKING_DIR, "/data/tweets_containing_only_policing_28_69.rds", sep=""))
head(data_sample1)
data_sample1
1+1
data_sample1[1,]
data <- data_sample1
data$created_at <- as.Date(data$created_at)
data_ <- data %>%
dplyr::mutate(period = if_else((created_at >= "2020-10-20" & created_at <= "2020-11-19"), "Period1",
if_else(created_at >= "2020-11-20" & created_at <= "2020-12-19", "Period2",
if_else(created_at >= "2020-12-20" & created_at <= "2021-01-19", "Period3", "0"))))
library(dplyr)
data_ <- data %>%
dplyr::mutate(period = if_else((created_at >= "2020-10-20" & created_at <= "2020-11-19"), "Period1",
if_else(created_at >= "2020-11-20" & created_at <= "2020-12-19", "Period2",
if_else(created_at >= "2020-12-20" & created_at <= "2021-01-19", "Period3", "0"))))
rm(list=ls()[! ls() %in%
c("data_")])
location <- read.csv(file=paste(WORKING_DIR, "/data/PoliceForce_location.csv", sep=""), sep=",", head=TRUE)
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'
location <- read.csv(file=paste(WORKING_DIR, "/data/PoliceForce_location.csv", sep=""), sep=",", head=TRUE)
head(location)
data1 = data_ %>%
dplyr::arrange(status_id) %>%
dplyr::filter(!duplicated(status_id))%>%
dplyr::filter(is_retweet==FALSE)%>% #remove retweets
dplyr::mutate(location1= gsub(",.*$", "", location)) %>% #add city name
dplyr::select(-c(location))%>%
dplyr::rename(location=location1)
data_1 <- data1 %>%
inner_join(location)
rm(list=ls()[! ls() %in%
c("data_1")])
Pf_names_regions <- read.table(file=paste(WORKING_DIR, "/data/Regions.csv", sep=""), sep=",", head=TRUE)
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'
Pf_names_regions <- read.table(file=paste(WORKING_DIR, "/data/Regions.csv", sep=""), sep=",", head=TRUE)
Pf_names_regions_uni <- Pf_names_regions$Police.Force
rm(list=ls()[! ls() %in%
c("WORKING_DIR", "data_1", "Pf_names_regions_uni", "fix.contractions", "removeSpecialChars")])
fix.contractions <- function(doc) {
# "won't" is a special case as it does not expand to "wo not"
doc <- gsub("won't", "will not", doc)
doc <- gsub("can't", "can not", doc)
doc <- gsub("n't", " not", doc)
doc <- gsub("'ll", " will", doc)
doc <- gsub("'re", " are", doc)
doc <- gsub("'ve", " have", doc)
doc <- gsub("'m", " am", doc)
doc <- gsub("'d", " would", doc)
doc <- gsub("\"", " ", doc)
doc <- gsub("\n", " ", doc)
# 's could be 'is' or could be possessive: it has no expansion
doc <- gsub("'s", "", doc)
return(doc)
}
#----------------------------------------------------
#function to remove special xters
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
periods <- unique(data_1$period)[2:4]
periods
i=1
tweet_container_afinn <- NULL
z = 1
Pf_names_regions_uni[i]!="Metropolitan Police"
placeTwt <- data_1 %>%
dplyr::filter(period == periods[z])%>%
dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
dplyr::select(text) %>%
dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
mutate(text=str_replace_all(text, "[[:punct:]]", " "))
library(stringr)
library(opitools)
library(twitteR) #for setting up Twitter authorization
library(rtweet) #for creating Twitter Authorization Token(S).
library(dplyr) #for data manipulation and analysis
library(colormap)
library(fmsb)
library(tidyr)
library(radarchart)
library(grDevices)
library(plotly)
library(webshot)
library(rgdal)
library(sf)
library(stringr)
library(tidytext)
library(reshape2)
library(stringdist)
library(tidyverse)
library(lubridate)
library(scales)
library(ggradar)
library(RColorBrewer)
library(ggpubr)
library(see)
library(likert)
library(rgdal)
library(rgeos)
library(maptools)
library(ggplot2)
library(scales)
placeTwt <- data_1 %>%
dplyr::filter(period == periods[z])%>%
dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
dplyr::select(text) %>%
dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
mutate(text=str_replace_all(text, "[[:punct:]]", " "))
#more cleaning
placeTwt <- unlist(placeTwt) %>% stringr::str_remove(pattern = "t co.*")
#to detect the similarities
placeTwt <- data.frame(cbind(text1=placeTwt, text2=c("NULL", placeTwt[1:length(placeTwt)-1])))
dim(placeTwt)
head(placeTwt)
#filter duplicates based on the similarities index
similarities <- stringsim(placeTwt$text1,placeTwt$text2,method='lcs', p=0.25)
placeTwt <- placeTwt[, 1]
dim(placeTwt) #head(placeTwt)
placeTwt <- placeTwt[which(similarities!=1)]
placeTwt <- data.frame(text=placeTwt)
#remove emoticons
placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
placeTwt$text <- sapply(placeTwt , fix.contractions)
#remove special characters
placeTwt$text <- sapply(placeTwt$text, removeSpecialChars)
#convert all text to lower case
placeTwt$text <- sapply(placeTwt$text, tolower)
placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
placeTwt
head(placeTwt )
tweet_container_afinn <- NULL
if(Pf_names_regions_uni[i]!="Metropolitan Police"){
#clean tweets
placeTwt <- data_1 %>%
dplyr::filter(period == periods[z])%>%
dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
dplyr::select(text) %>%
dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
mutate(text=str_replace_all(text, "[[:punct:]]", " "))
}
#more cleaning
placeTwt <- unlist(placeTwt) %>% stringr::str_remove(pattern = "t co.*")
#to detect the similarities
placeTwt <- data.frame(cbind(text1=placeTwt, text2=c("NULL", placeTwt[1:length(placeTwt)-1])))
dim(placeTwt)
head(placeTwt)
#filter duplicates based on the similarities index
similarities <- stringsim(placeTwt$text1,placeTwt$text2,method='lcs', p=0.25)
placeTwt <- placeTwt[, 1]
dim(placeTwt) #head(placeTwt)
placeTwt <- placeTwt[which(similarities!=1)]
placeTwt <- data.frame(text=placeTwt)
#remove emoticons
placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#changing expanding meaningful abbreviations
placeTwt$text <- sapply(placeTwt , fix.contractions)
#remove special characters
placeTwt$text <- sapply(placeTwt$text, removeSpecialChars)
#convert all text to lower case
placeTwt$text <- sapply(placeTwt$text, tolower)
#more removals
placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#--------------------------
placeTwt
placeTwt[1,]
mode(placeTwt)
placeTwt  <- as.data.frame(placeTwt)
head(placeTwt)
#detect tweets with pandemic keywords
placeTwt_keyword_exist <- data.frame(placeTwt$text) %>%
dplyr::filter(stringr::str_detect(text,'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=FALSE))
#detect tweets with no
placeTwt_no_keyword <- data.frame(placeTwt$text) %>%
dplyr::filter(stringr::str_detect(text, 'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=TRUE))
#create the id field and drop text field
placeTwt_keyword_exist$ID <- seq.int(nrow(placeTwt_keyword_exist))
#do sentiment analysis and also detect negated sentiment for
#tweets with pandemic keywords
token_neg_exist <- as_tibble(placeTwt_keyword_exist) %>%
#unnest_tokens(word, text)%>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
separate(bigram, c("word1", "word2"), sep = " ")%>%
as.data.frame() %>%
#filter the preceeding by the 'negation' word
dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
dplyr::rename(word  = word2)%>%
left_join(get_sentiments("afinn")) %>% #
dplyr::select(ID, neg, value)%>%
dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
dplyr::mutate(value = value2)%>%
dplyr::select(-c(value2))%>%
dplyr::rename(word=neg)%>%
dplyr::filter(!is.na(value))%>%
mutate(pandem_keyword="exist")%>%
mutate(country=Pf_names_regions_uni[i])%>%
mutate(Period = periods[z])
placeTwt_all_DATA_afinn_exist <- placeTwt_keyword_exist %>%
unnest_tokens(word, text, drop = FALSE) %>%
inner_join(get_sentiments("afinn")) %>% #join with the lexicon
mutate(pandem_keyword="exist")%>%
mutate(country=Pf_names_regions_uni[i])%>%
mutate(Period = periods[z])%>%
dplyr::select(-c(text))#drop text
#create the id field and drop text field
placeTwt_all_DATA_afinn_exist <- data.frame(rbind(placeTwt_all_DATA_afinn_exist,
token_neg_exist))
#create the sentiment data (bing)
placeTwt_no_keyword$ID <- seq.int(nrow(placeTwt_no_keyword)) #create an id field
#get an ngram (2-ngrame)
#do sentiment analysis and also detect negated sentiment for
#tweets with no pandemic keywords
token_neg_no_keyword <- as_tibble(placeTwt_no_keyword) %>%
#unnest_tokens(word, text)%>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
separate(bigram, c("word1", "word2"), sep = " ")%>%
as.data.frame() %>%
#filter the preceeding by the 'negation' word
dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
dplyr::rename(word  = word2)%>%
left_join(get_sentiments("afinn")) %>% #
dplyr::select(ID, neg, value)%>%
dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
dplyr::mutate(value = value2)%>%
dplyr::select(-c(value2))%>%
dplyr::rename(word=neg)%>%
dplyr::filter(!is.na(value))%>%
mutate(pandem_keyword="exist")%>%
mutate(country=Pf_names_regions_uni[i])%>%
mutate(Period = periods[z])
placeTwt_all_DATA_afinn_absent <- placeTwt_no_keyword %>%
unnest_tokens(word, text, drop = FALSE) %>%
inner_join(get_sentiments("afinn")) %>% #join with the lexicon
mutate(pandem_keyword="absent")%>%
mutate(country=Pf_names_regions_uni[i])%>%
mutate(Period = periods[z])%>%
dplyr::select(-c(text))
#join the two observed sentiment document (OSD) documents
placeTwt_all_DATA_afinn_absent <- data.frame(rbind(placeTwt_all_DATA_afinn_absent,
token_neg_no_keyword))
placeTwt_all_DATA_afinn_observed <- data.frame(rbind(placeTwt_all_DATA_afinn_exist, placeTwt_all_DATA_afinn_absent))
tweet_container_afinn <- rbind(tweet_container_afinn, placeTwt_all_DATA_afinn_observed)
tweet_container_afinn
write.table(tweet_container_afinn, file = paste(WORKING_DIR, "/outputs/",
"police_cleaned_", Pf_names_regions_uni[i], "_all_DATA_afinn_observed_ts",
".csv", sep=""), sep=",", row.names = F)
regions <- c("North West", "North East","Yorkshire and the Humber",
"West Midlands","East Midlands","Eastern",
"Wales", "South West","South East")
t1 <- Sys.time()
op <- par(mfrow = c(2,2),
oma = c(5,4,0,0) + 0.5,
mar = c(0,0,4,4) + 0.5)
#all_UK_bing_combined <- NULL
all_Regional_afinn_combined <- NULL
all_pvalueS_1 <- NULL
all_pvalueS_2 <- NULL
all_pvalueS_3 <- NULL
all_p_signs_1 <- matrix(0, 42, 3)
all_p_signs_2 <- matrix(0, 42, 3)
all_p_signs_3 <- matrix(0, 42, 3)
init <- 0
i=1
subsetP <- Pf_names_regions %>%
filter(Regions == regions[i])
Pf_names_regions
Pf_names_regions <- read.table(file=paste(WORKING_DIR, "/data/Regions.csv", sep=""), sep=",", head=TRUE)
subsetP <- Pf_names_regions %>%
filter(Regions == regions[i])
Region <- regions[i]
#UK_bing_combined <- NULL
Regional_afinn_combined <- NULL
j=1
init <- init + 1
#read in the OSD
UK_afinn_read <- read.table(file = paste(WORKING_DIR, "/outputs/",
"police_cleaned_", subsetP$Police.Force[j],"_all_DATA_afinn_observed_ts",
".csv", sep=""), sep=",", head=TRUE)
j
subsetP$Police.Force[j]
subsetP
subsetP[1,]
subsetP[2,]
subsetP[3,]
subsetP[4,]
subsetP[5,]
subsetP[6,]
Pf_names_regions
Pf_names_regions[1,]
regions[i]
i=2
Pf_names_regions %>%
filter(Regions == regions[i])
Pf_names_regions
Pf_names_regions[1,]
subsetP <- Pf_names_regions %>%
filter(Regions == regions[i])
Region <- regions[i]
#UK_bing_combined <- NULL
Regional_afinn_combined <- NULL
init <- init + 1
#read in the OSD
UK_afinn_read <- read.table(file = paste(WORKING_DIR, "/outputs/",
"police_cleaned_", subsetP$Police.Force[j],"_all_DATA_afinn_observed_ts",
".csv", sep=""), sep=",", head=TRUE)
UK_afinn_read
UK_afinn_read[1,]
#compute a unique sentiment for each tweet
UK_afinn_read <- UK_afinn_read %>%
dplyr::group_by(Period, country, pandem_keyword, ID)%>%
dplyr::summarise(sentiment_score = sum(value))%>%
dplyr::filter(sentiment_score != 0) %>%
dplyr::mutate(sentiment = if_else(sentiment_score > 0, "positive", "negative"))%>%
dplyr::select(-c(sentiment_score))%>%
ungroup %>%
dplyr::mutate(word = "word")%>%
dplyr::select(ID, word, sentiment, pandem_keyword,country,Period)
head(UK_afinn_read)
unique(UK_afinn_read$pandem_keyword)
unique(UK_afinn_read$Period)
#then plot the expected and observed.
periods <- unique(UK_afinn_read$Period)
container2_holder <- NULL
net_sentiment_simulation <- function(data, replicas=999){
simulated_esd <- NULL
for(m in 1:replicas){
p1 <- data %>%
#dplyr::filter(pandem_keyword=="absent")%>%
group_by(pandem_keyword)%>%
mutate(nnrow=length(word))%>%
mutate(prob1=nnrow/nrow(data))%>% #prob of absent
group_by(pandem_keyword, sentiment) %>%
mutate(pos_neg_count=n())%>%
mutate(prob2=pos_neg_count/nnrow)##%>%
ab_class_sent <- p1[which(p1$pandem_keyword == "absent"),3]
length_exist_group <- length(which(p1$pandem_keyword == "exist"))
#now collate the unique probabilities of 'absent' class
p1_prob <- p1 %>%
dplyr::filter(pandem_keyword == "absent")%>%
dplyr::distinct(sentiment, .keep_all = TRUE)%>%
dplyr::select(sentiment, prob2)
if(replicas == 1){
set.seed(nrow(data))
new_ex_class_sent = sample(p1_prob$sentiment, length_exist_group, replace=TRUE, prob = p1_prob$prob2)
}
if(replicas > 1){
if(m == 1){
set.seed(nrow(data))
new_ex_class_sent = sample(p1_prob$sentiment, length_exist_group, replace=TRUE, prob = p1_prob$prob2)
}
if(m > 1){
new_ex_class_sent = sample(p1_prob$sentiment, length_exist_group, replace=TRUE, prob = p1_prob$prob2)
}
}
new_ex_class_sent
new_sentiment_list <- c(new_ex_class_sent, as.vector(unlist(ab_class_sent)))
final_p1 <- data.frame(cbind(p1, sentiment2=new_sentiment_list))
#expected
UK_bing_ESD <- final_p1 %>%
dplyr::select(ID, word, sentiment, pandem_keyword, country, sentiment2)%>%
mutate(sentiment = sentiment2)%>%
dplyr::select(-c(sentiment2))
UK_bing_ESD <-  UK_bing_ESD %>%
#filter(country != "NA" & !sentiment %in% c("positive", "negative")) %>%
count(sentiment, country) %>%
group_by(country, sentiment) %>%
summarise(sentiment_sum = sum(n)) %>%
ungroup()
UK_bing_ESD_ = UK_bing_ESD %>%
group_by(country) %>%
dplyr::mutate(total=sum(sentiment_sum))%>%
mutate(pct=round((sentiment_sum/total)*100, digits=2))
UK_bing_ESD_ = data.frame(dcast(UK_bing_ESD_, sentiment ~ country))
UK_bing_2_ESD = UK_bing_ESD_ %>% gather(Country, valname, -sentiment) %>%
spread(sentiment, valname)%>%
dplyr::rename(negative_ESD = negative)%>%
dplyr::rename(positive_ESD=positive) %>%
mutate(net_sentiment_ESD = positive_ESD - negative_ESD)
simulated_esd <- rbind(simulated_esd, UK_bing_2_ESD)
}
return(simulated_esd)
}
periods <- unique(UK_afinn_read$Period)
container2_holder <- NULL
for(y in 1:length(periods)){  #y<-1
UK_afinn_read_subset <- UK_afinn_read %>%
dplyr::filter(Period == periods[y])
#--------------------------------------
#randomization testing (replicas can be reduced to e.g. 99 for runtime)
esd_simulated <- net_sentiment_simulation(data = UK_afinn_read_subset, replicas=99)
#compute the observed from the original data
UK_afinn_OSD <- UK_afinn_read_subset %>%
dplyr::select(ID, word, sentiment, pandem_keyword, country)
UK_afinn_OSD <-  UK_afinn_OSD %>%
#filter(country != "NA" & !sentiment %in% c("positive", "negative")) %>%
count(sentiment, country) %>%
group_by(country, sentiment) %>%
summarise(sentiment_sum = sum(n)) %>% #here is where to deduct the negation
ungroup()
#calculate % sentiment
UK_afinn_OSD_ = UK_afinn_OSD %>%
group_by(country) %>%
dplyr::mutate(total=sum(sentiment_sum))%>%
mutate(pct=round((sentiment_sum/total)*100, digits=2))
UK_afinn_OSD_ = data.frame(dcast(UK_afinn_OSD_, sentiment ~ country))
UK_afinn_2_OSD = UK_afinn_OSD_ %>% gather(Country, valname, -sentiment) %>%
spread(sentiment, valname)%>%
dplyr::rename(negative_OSD = negative)%>%
dplyr::rename(positive_OSD=positive) %>%
mutate(net_sentiment_OSD =  positive_OSD -  negative_OSD)
#contruct the confidence interval (ave +/- z * se) and compute p-value
if(y == 1){
if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
all_p_signs_1[init, 1] <- "TRUE"
}
all_p_signs_1[init, 2] <- subsetP$Police.Force[j]
all_p_signs_1[init, 3] <- regions[i]
}
if(y == 2){
if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
all_p_signs_2[init, 1] <- "TRUE"
}
all_p_signs_2[init, 2] <- subsetP$Police.Force[j]
all_p_signs_2[init, 3] <- regions[i]
}
if(y == 3){
if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
all_p_signs_3[init, 1] <- "TRUE"
}
all_p_signs_3[init, 2] <- subsetP$Police.Force[j]
all_p_signs_3[init, 3] <- regions[i]
}
beat_left <- length(which(esd_simulated$net_sentiment_ESD >  UK_afinn_2_OSD$net_sentiment_OSD))
beat_right <- length(which(esd_simulated$net_sentiment_ESD <  UK_afinn_2_OSD$net_sentiment_OSD))
Sbeat <- min(c(beat_left, beat_right))
pvalue_sentiment <- (Sbeat + 1)/(length(esd_simulated$net_sentiment_ESD) + 1)
if(y == 1){
all_pvalueS_1 <- rbind(all_pvalueS_1, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
}
if(y == 2){
all_pvalueS_2 <- rbind(all_pvalueS_2, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
}
if(y == 3){
all_pvalueS_3 <- rbind(all_pvalueS_3, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
}
mean_exp <- data.frame(matrix(colMeans(esd_simulated[,2:ncol(esd_simulated)]),1,))
colnames(mean_exp)<-colnames(esd_simulated)[2:length(colnames(esd_simulated))]
#combine periods data
container2_holder <- rbind(container2_holder,
cbind(UK_afinn_2_OSD, mean_exp)%>% mutate(Period=periods[y]))
}
container2_holder
container2_holder[1:5,]
nrow(container2_holder)
#combine the regional data
Regional_afinn_combined <- rbind(Regional_afinn_combined, container2_holder)
all_Regional_afinn_combined <- rbind(all_Regional_afinn_combined, Regional_afinn_combined %>%
mutate(Region = regions[i]))
flush.console()
print(i)
all_Regional_afinn_combined[1,]
