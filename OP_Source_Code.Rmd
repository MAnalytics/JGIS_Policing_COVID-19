---
title: "An Analytical Framework for Measuring Inequalities in the Public Opinions on Policing – "
subtitle: "Assessing COVID-19 Pandemic in Space-Time using Twitter Data"
author: "Monsuru Adepeju"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    # keep_tex: yes
    # latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

_This is the accompanying source code for the paper 'An Analytical Framework for Measuring Inequalities in the Public Opinions on Policing – Assessing COVID-19 Pandemic in Space-Time using Twitter Data', submitted to JGIS_

# Introduction

The paper (mentioned above) proposes an analytical framework for measuring the inequalities in the public opinion on policing. The aim of this document is to allow users to replicate the outputs as presented in the paper using their own datasets. The document is divided into different sections which complete different component of the analysis.

## Configuration and library loading

Configure the script here. As well as making sure all the libraries used below have been installed, you need to:

 1. Set your working directory (the `WORKING_DIR` variable below) to be the location of this script.
 2. Install all the libraries required to complete the analysis.
 

```{r initialise, message=FALSE}

# This directory needs to be set to the location of this script
WORKING_DIR <- 'C:/R/Github/JGIS_Policing_COVID-19'
setwd(WORKING_DIR)

library(twitteR) #for setting up Twitter authorization
library(rtweet) #for creating Twitter Authorization Token(S).
library(dplyr) #for data manipulation and analysis
library(colormap)
library(fmsb)
library(tidyr)
library(radarchart)
library(grDevices)
library(plotly)
library(webshot)
library(rgdal)
library(sf)
library(stringr)
library(tidytext)
library(reshape2)
library(stringdist)
library(tidyverse)
library(lubridate)
library(scales)
library(ggradar)
library(RColorBrewer)
library(ggpubr)
library(see)
library(viridis)
library(likert)

```
 
# Downloading Twitter Data

All new developers must apply for a developer account to access Twitter APIs (see [here](https://developer.twitter.com/en/apply-for-access)). Once approved, you follow the instruction on this [page](https://developer.twitter.com/en/docs/twitter-api/getting-started/guide). Within the developer App, you will be provided a set of API Keys (also known as Consumer Keys). You will also have the chance to generate a set of Access Tokens that can be used to make tweet requests, and a Bearer Token that can be used to authenticate endpoints. Below, we demonstrate how we used one of the Author's keys and token to download tweets within a defined geographical coverage. The tweets needed for this analysis are tweets that include the keywords or hashtages: 'police', 'policing', and 'law enforcement(s)' (Note: the `eval=FALSE` argument below is to skip the chunk of the script). Modify the chunk as appropriate.


```{r, message=FALSE, eval=FALSE}

#define keys and tokens (Note: real keys or tokens do not contain asterisk as below)
consumer_key <- 'rJWorDnMoARYE7********' 
consumer_secret <- 'dUJktwlOwdbaUNB15z2Yw4HI3p*************'
access_token <- '1108852279434715136-4IpdsZ***************'
access_secret <- 'eX78KEUNtaU6GZ0wFay5laf******************'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

token <- create_token(
  app = "UK2019", #App name
  consumer_key = consumer_key,
  consumer_secret = consumer_secret)

#define the keywords to search in tweets
keywords <- c("police", "policing", "law enforcement", "law enforcements")

#tweets holder
all_Tweets <- NULL

#Given a party  
for(i in seq_len(length(keywords))){ #i<-1
  
  tweets_g1 <- NULL
  #tweets_g2 <- NULL
  
  tweets_g1 <- search_tweets(q=keywords[i],  n=17500, type="recent", include_rts=TRUE, 
                             token = token, lang="en",geocode='53.805,-4.242,350mi') 
  #the lat, long and rad above defines the geographical coverage within which the download is performed.
  if(nrow(tweets_g1)!=0){
    tweets_g1 <- tweets_g1 %>% dplyr::mutate(class=keywords[i])
    all_Tweets <- rbind(all_Tweets, tweets_g1)  #all_Tweets<-NULL
  }

  flush.console()
  print(paste(nrow(tweets_g1), nrow(tweets_g1), sep="||"))
  print("waiting for 15.5 minutes")
  testit(960) #wait for another 15 minutes before downloading again
}

#save the results
write_as_csv(all_Tweets, paste("only_policeTweet_set_", 1, "_.csv", sep=""), na="NA", fileEncoding = "UTF-8")
#change the value "1" for subsequent downloading.

```

# Geocoding

The downloaded tweet files are read into RStudio and appended. Next, we geocode each tweets to its respective police force areas (PFA) using the [`PFA-Location lookup`](./data/PoliceForce_location.csv) table. Import other necessary files.

```{r, eval = FALSE}
#read in downloaded tweets
data_sample1 <- read.table(file="only_policeTweet_set_1.csv", sep=",", head=TRUE)
.
.
data_sampleN <- read.table(file="only_policeTweet_set_N.csv", sep=",", head=TRUE)

#combine data
data <- rbind(data_sample1, ....., data_sampleN)

#format date field
data$created_at <- as.Date(data$created_at)

#create and append time steps (or periods)
data_ <- data %>% 
  dplyr::mutate(period = if_else((created_at >= "2020-10-20" & created_at <= "2020-11-19"), "Period1",
                                 if_else(created_at >= "2020-11-20" & created_at <= "2020-12-19", "Period2",
                                         if_else(created_at >= "2020-12-20" & created_at <= "2021-01-19", "Period3", "0"))))             
#house cleaning
rm(list=ls()[! ls() %in% 
  c("data_")])

#read in the PFA-location lookup table
location <- read.csv(file="PoliceForce_location.csv", sep=",", head=TRUE)

#preview
head(location)
#check length
nrow(location)

#summary of location table
table(location$policeForce)

#Subset only the tweets needed (i.e. Organic tweets and Replies)
data1 = data_ %>%
  dplyr::arrange(status_id) %>%
  dplyr::filter(!duplicated(status_id))%>%
  dplyr::filter(is_retweet==FALSE)%>% #remove retweets
  dplyr::mutate(location1= gsub(",.*$", "", location)) %>% #add city name
  dplyr::select(-c(location))%>%
  dplyr::rename(location=location1)

head(data1)

#geocoding
data_1 <- data1 %>% 
  inner_join(location)

#house cleaning
rm(list=ls()[! ls() %in% 
               c("data_1")])

#get names of policing regions
Pf_names_regions <- read.table(file="Regions.csv", sep=",", head=TRUE)

#collate names of PFAs
Pf_names_regions_uni <- Pf_names_regions$Police.Force

#house cleaning
rm(list=ls()[! ls() %in% 
               c("data_1", "Pf_names_regions_uni", "fix.contractions", "removeSpecialChars")])

#get the time steps
periods <- unique(data_1$period)[2:4]

```


# Data Cleaning, Tidying and Sentiment Analysis

```{r, eval=FALSE}

#given a police force area
#--------------------------------
for(i in 1:length(Pf_names_regions_uni)){ #loop through policing regions

  tweet_container_afinn <- NULL
  
  #given a time step (period)
  for(z in 1:length(periods)){ #z = 1
    
    #use sample of London tweets (25%)
    if(Pf_names_regions_uni[i]=="Metropolitan Police"){
      #clean tweets
      placeTwt <- data_1 %>% 
        dplyr::filter(period == periods[z])%>%
        dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
        dplyr::select(text) %>%
        dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
        mutate(text=str_replace_all(text, "[[:punct:]]", " "))
        set.seed(1000)
        placeTwt <- sample_frac(placeTwt, 0.25)
    }
    
    if(Pf_names_regions_uni[i]!="Metropolitan Police"){
      #clean tweets
      placeTwt <- data_1 %>% 
        dplyr::filter(period == periods[z])%>%
        dplyr::filter(policeForce==Pf_names_regions_uni[i]) %>%
        dplyr::select(text) %>%
        dplyr::mutate(text = gsub("http://*|https://*|https*|\n*|*>|<*","", text)) %>%
        mutate(text=str_replace_all(text, "[[:punct:]]", " "))
    }

    #more cleaning
    placeTwt <- unlist(placeTwt) %>% stringr::str_remove(pattern = "t co.*")
    
    #to detect the similarities
    placeTwt <- data.frame(cbind(text1=placeTwt, text2=c("NULL", placeTwt[1:length(placeTwt)-1])))
    dim(placeTwt)
    head(placeTwt)
    
    #filter duplicates based on the similarities index
    similarities <- stringsim(placeTwt$text1,placeTwt$text2,method='lcs', p=0.25)
    
    placeTwt <- placeTwt[, 1]
    dim(placeTwt) #head(placeTwt)
    placeTwt <- placeTwt[which(similarities!=1)]
    
    placeTwt <- data.frame(text=placeTwt)
    
    #remove emoticons
    placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
    
    placeTwt$text <- sapply(placeTwt , fix.contractions)
    
    #remove special characters
    placeTwt$text <- sapply(placeTwt$text, removeSpecialChars)
    
    #convert all text to lower case
    placeTwt$text <- sapply(placeTwt$text, tolower)
    
    placeTwt$text <- sapply(placeTwt,function(row) iconv(row, "latin1", "ASCII", sub=""))
    
    #detect tweets with pandemic keywords
    placeTwt_keyword_exist <- data.frame(placeTwt$text) %>%
    dplyr::filter(stringr::str_detect(text,'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=FALSE))
    #detect tweets with no pandemic keywords
    placeTwt_no_keyword <- data.frame(placeTwt$text) %>%
      dplyr::filter(stringr::str_detect(text, 'pandemic|pandemics|lockdown|lockdowns|corona|coronavirus|covid|covid19|covid-19|virus|viruses|quarantine|infect|infects|infecting|infected', negate=TRUE))
    
    #create the id field and drop text field
    placeTwt_keyword_exist$ID <- seq.int(nrow(placeTwt_keyword_exist)) 
    
    #do sentiment analysis and also detect negated sentiment for 
    #tweets with pandemic keywords
    token_neg_exist <- as_tibble(placeTwt_keyword_exist) %>%
      #unnest_tokens(word, text)%>%
      unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
      separate(bigram, c("word1", "word2"), sep = " ")%>%
      as.data.frame() %>%
      #filter the preceeding by the 'negation' word
      dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
      dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
      dplyr::rename(word  = word2)%>%
      left_join(get_sentiments("afinn")) %>% #
      dplyr::select(ID, neg, value)%>%
      dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
      dplyr::mutate(value = value2)%>%
      dplyr::select(-c(value2))%>%
      dplyr::rename(word=neg)%>%
      dplyr::filter(!is.na(value))%>%
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])
    
    placeTwt_all_DATA_afinn_exist <- placeTwt_keyword_exist %>%
      unnest_tokens(word, text, drop = FALSE) %>%
      inner_join(get_sentiments("afinn")) %>% #join with the lexicon
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])%>% 
      dplyr::select(-c(text))#drop text
    
    #create the id field and drop text field
    placeTwt_all_DATA_afinn_exist <- data.frame(rbind(placeTwt_all_DATA_afinn_exist,
                                                      token_neg_exist))
  
    #create the sentiment data (bing)
    placeTwt_no_keyword$ID <- seq.int(nrow(placeTwt_no_keyword)) #create an id field
    #get an ngram (2-ngrame)
    
    #do sentiment analysis and also detect negated sentiment for 
    #tweets with no pandemic keywords
    token_neg_no_keyword <- as_tibble(placeTwt_no_keyword) %>%
      #unnest_tokens(word, text)%>%
      unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
      separate(bigram, c("word1", "word2"), sep = " ")%>%
      as.data.frame() %>%
      #filter the preceeding by the 'negation' word
      dplyr::filter(word1 %in% c("not", "never", "no", "without")) %>%
      dplyr::mutate(neg = paste(word1, word2, sep=" ")) %>% #get sentiment score
      dplyr::rename(word  = word2)%>%
      left_join(get_sentiments("afinn")) %>% #
      dplyr::select(ID, neg, value)%>%
      dplyr::mutate(value2 = value * 2)%>%#reverse the score (and multiply by 2)
      dplyr::mutate(value = value2)%>%
      dplyr::select(-c(value2))%>%
      dplyr::rename(word=neg)%>%
      dplyr::filter(!is.na(value))%>%
      mutate(pandem_keyword="exist")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])
    
    placeTwt_all_DATA_afinn_absent <- placeTwt_no_keyword %>%
      unnest_tokens(word, text, drop = FALSE) %>%
      inner_join(get_sentiments("afinn")) %>% #join with the lexicon
      mutate(pandem_keyword="absent")%>%
      mutate(country=Pf_names_regions_uni[i])%>%
      mutate(Period = periods[z])%>%
      dplyr::select(-c(text))
    
    #join the two observed sentiment document (OSD) documents
    placeTwt_all_DATA_afinn_absent <- data.frame(rbind(placeTwt_all_DATA_afinn_absent,
                                                       token_neg_no_keyword))
    
    placeTwt_all_DATA_afinn_observed <- data.frame(rbind(placeTwt_all_DATA_afinn_exist, placeTwt_all_DATA_afinn_absent))
    
    tweet_container_afinn <- rbind(tweet_container_afinn, placeTwt_all_DATA_afinn_observed)
    
  }
  
  flush.console()
  print(paste(i, z, " | "))
  
  #write the OSD for the current PFA
  write.table(tweet_container_afinn, file = paste("C:/Users/55131065/Desktop/downloadTweets/outputs/", 
                                                  "police_cleaned_", Pf_names_regions_uni[i], "_all_DATA_afinn_observed_ts", 
                                                  ".csv", sep=""),
              sep=",", row.names = F)
  
}

```


# Compute the Opinion Scores and p-values

This chunk computes:

i. the opinion score of each tweet, 
ii. then resultant opinion scores of each PFA
iii. Derive the expected sentiment document (ESD)
iv. Do randomization testing and compute the p-values

The key outputs of this chunk are the:

a. `Observation` tables showing the computed OP scores across PFAs and time steps, 
b.  `P-value` tables showing the statistical significant values based on N replications, and the 
c. `Position` table that describe the position of an observed OP score relative to the mean expectation.

```{r, eval=FALSE}

#names of policing region
regions <- c("North West", "North East","Yorkshire and the Humber",
             "West Midlands","East Midlands","Eastern",
             "Wales", "South West","South East")

#start
t1 <- Sys.time()

op <- par(mfrow = c(2,2),
          oma = c(5,4,0,0) + 0.5,
          mar = c(0,0,4,4) + 0.5)


#all_UK_bing_combined <- NULL
all_Regional_afinn_combined <- NULL
all_pvalueS_1 <- NULL
all_pvalueS_2 <- NULL
all_pvalueS_3 <- NULL

all_p_signs_1 <- matrix(0, 42, 3)
all_p_signs_2 <- matrix(0, 42, 3)
all_p_signs_3 <- matrix(0, 42, 3)

init <- 0
#generate the expected and observed for all regions

#loop throug region
for(i in 1:length(regions)){ #i=1

  subsetP <- Pf_names_regions %>% 
    filter(Regions == regions[i])
  
  Region <- regions[i]
  
  #UK_bing_combined <- NULL
  Regional_afinn_combined <- NULL
  
  for(j in 1:nrow(subsetP)) {#j=1
    
    init <- init + 1
    
    #read in the OSD
    UK_afinn_read <- read.table(file = paste("C:/Users/55131065/Desktop/downloadTweets/outputs/", 
                                             "police_cleaned_", subsetP$Police.Force[j],"_all_DATA_afinn_observed_ts", 
                                             ".csv", sep=""), sep=",", head=TRUE)
    
    #compute a unique sentiment for each tweet
    UK_afinn_read <- UK_afinn_read %>% 
      dplyr::group_by(Period, country, pandem_keyword, ID)%>%
      dplyr::summarise(sentiment_score = sum(value))%>%
      dplyr::filter(sentiment_score != 0) %>%
      dplyr::mutate(sentiment = if_else(sentiment_score > 0, "positive", "negative"))%>%
      dplyr::select(-c(sentiment_score))%>%
      ungroup %>%
      dplyr::mutate(word = "word")%>%
      dplyr::select(ID, word, sentiment, pandem_keyword,country,Period)
    
    head(UK_afinn_read)
    
    unique(UK_afinn_read$pandem_keyword)
    unique(UK_afinn_read$Period)
    #word sentiment pandem_keyword  country
  
    #create Expected Ssentiment Document
    #compute the probability of observed from the 'absent' group
    #generate the sentiment list based on the prob.
    #assign the 'exist' group with new sentiments based on the prob
    #then plot the expected and observed.
    
    periods <- unique(UK_afinn_read$Period)
    
    container2_holder <- NULL
    
    for(y in 1:length(periods)){  #y<-1
      
      UK_afinn_read_subset <- UK_afinn_read %>%
        dplyr::filter(Period == periods[y])
      
      #--------------------------------------
      #randomization testing (replicas can be reduced to e.g. 99 for runtime)
      esd_simulated <- net_sentiment_simulation(data = UK_afinn_read_subset, replicas=999)
      
      
      #compute the observed from the original data
      UK_afinn_OSD <- UK_afinn_read_subset %>% 
        dplyr::select(ID, word, sentiment, pandem_keyword, country)

      UK_afinn_OSD <-  UK_afinn_OSD %>%
        #filter(country != "NA" & !sentiment %in% c("positive", "negative")) %>%
        count(sentiment, country) %>%
        group_by(country, sentiment) %>%
        summarise(sentiment_sum = sum(n)) %>% #here is where to deduct the negation
        ungroup()
      
      #calculate % sentiment
      UK_afinn_OSD_ = UK_afinn_OSD %>% 
        group_by(country) %>%
        dplyr::mutate(total=sum(sentiment_sum))%>%
        mutate(pct=round((sentiment_sum/total)*100, digits=2))
      
      UK_afinn_OSD_ = data.frame(dcast(UK_afinn_OSD_, sentiment ~ country))
      
      UK_afinn_2_OSD = UK_afinn_OSD_ %>% gather(Country, valname, -sentiment) %>% 
        spread(sentiment, valname)%>%
        dplyr::rename(negative_OSD = negative)%>%
        dplyr::rename(positive_OSD=positive) %>%
        mutate(net_sentiment_OSD =  positive_OSD -  negative_OSD)
      
      #contruct the confidence interval (ave +/- z * se) and compute p-value
      if(y == 1){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_1[init, 1] <- "TRUE"
        }
        all_p_signs_1[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_1[init, 3] <- regions[i]
      }
      
      if(y == 2){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_2[init, 1] <- "TRUE"
        }
        all_p_signs_2[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_2[init, 3] <- regions[i]
      }
      
      if(y == 3){
        if(UK_afinn_2_OSD$net_sentiment_OSD > mean(esd_simulated$net_sentiment_ESD)){
          all_p_signs_3[init, 1] <- "TRUE"
        }
        all_p_signs_3[init, 2] <- subsetP$Police.Force[j]
        all_p_signs_3[init, 3] <- regions[i]
      }
      
      beat_left <- length(which(esd_simulated$net_sentiment_ESD >  UK_afinn_2_OSD$net_sentiment_OSD))
      beat_right <- length(which(esd_simulated$net_sentiment_ESD <  UK_afinn_2_OSD$net_sentiment_OSD))
      Sbeat <- min(c(beat_left, beat_right))
      pvalue_sentiment <- (Sbeat + 1)/(length(esd_simulated$net_sentiment_ESD) + 1)
      
      if(y == 1){
        all_pvalueS_1 <- rbind(all_pvalueS_1, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }
      
      if(y == 2){
        all_pvalueS_2 <- rbind(all_pvalueS_2, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }
      
      if(y == 3){
        all_pvalueS_3 <- rbind(all_pvalueS_3, cbind(pvalue_sentiment, subsetP$Police.Force[j], regions[i]))
      }

      mean_exp <- data.frame(matrix(colMeans(esd_simulated[,2:ncol(esd_simulated)]),1,))
      colnames(mean_exp)<-colnames(esd_simulated)[2:length(colnames(esd_simulated))]
      
      #combine periods data
      container2_holder <- rbind(container2_holder, 
                                 cbind(UK_afinn_2_OSD, mean_exp)%>% mutate(Period=periods[y]))
    }
    
    #combine the regional data
    Regional_afinn_combined <- rbind(Regional_afinn_combined, container2_holder)
    
    
  }
  
  all_Regional_afinn_combined <- rbind(all_Regional_afinn_combined, Regional_afinn_combined %>%
                                         mutate(Region = regions[i]))
  
  flush.console()
  print(i)
}

#end time
t2 <- Sys.time()

#run time
t_diff <- t2 -t1

#Previewing the results
#read.table

#Export the 'Observation' table (Opinion scores for time steps 1, 2, 3, called 'Periods')
write.table(all_Regional_afinn_combined, file="all_obs_exp_Sentiment_ts_1_2_3_ALL.csv", sep=",", row.names = F)

#P-values and position table for time step 1
write.table(all_pvalueS_1, file="all_pvalue_Sentiment_ts_1.csv", sep=",", row.names = F)
write.table(all_p_signs_1, file="all_sentiment_signs_ts_1.csv", sep=",", row.names = F)

#P-values and position table for time step 2
write.table(all_pvalueS_2, file="all_pvalue_Sentiment_ts_2.csv", sep=",", row.names = F)
write.table(all_p_signs_2, file="all_sentiment_signs_ts_2.csv", sep=",", row.names = F)

#P-values and position table for time step 3
write.table(all_pvalueS_3, file="all_pvalue_Sentiment_ts_3.csv", sep=",", row.names = F)
write.table(all_p_signs_3, file="all_sentiment_signs_ts_3.csv", sep=",", row.names = F)



```


# Plotting

In this section, we provide codes for visualizing the results based on the tables generated above.

### Radar chart

To plot the measured opinion across the police force areas (PFAs)

```{r, eval=FALSE}

#define names and order of regions
regions <- c("North West", "North East","Yorkshire and the Humber",
             "West Midlands","East Midlands","Eastern",
             "Wales", "South West","South East")


#read in observation table
all_UK_bing_combined <- read.table(file="all_obs_exp_Sentiment_ts_1_2_3_ALL_6.csv", sep=",", head=TRUE)

#read in 'Position' tables
all_pvalueS_1 <- read.table(file="all_pvalue_Sentiment_ts_1.csv", sep=",", head=TRUE)
all_pvalueS_2 <- read.table(file="all_pvalue_Sentiment_ts_2.csv", sep=",", head=TRUE)
all_pvalueS_3 <- read.table(file="all_pvalue_Sentiment_ts_3.csv", sep=",", head=TRUE)

#read in P-values
all_p_signs_1 <- read.table(file="all_sentiment_signs_ts_1.csv", sep=",", head=TRUE)
all_p_signs_2 <- read.table(file="all_sentiment_signs_ts_2.csv", sep=",", head=TRUE)
all_p_signs_3 <- read.table(file="all_sentiment_signs_ts_3.csv", sep=",", head=TRUE)


for(k in 1:length(regions)){ 
  
  
  UK_bing_combined_subset <- all_UK_bing_combined[which(all_UK_bing_combined$Region %in% regions[k]),] %>%
    select(-c(Region))
  
  UK_bing_combined_subset <- UK_bing_combined_subset %>%
    dplyr::select(c(Country, Period, net_sentiment_OSD))
  
  UK_bing_combined_subset <- t(dcast(UK_bing_combined_subset, Period ~ Country))
  
  UK_bing_combined_subset <- UK_bing_combined_subset[2:nrow(UK_bing_combined_subset), ]
  
  UK_bing_combined_subset <- data.frame(cbind(rownames(UK_bing_combined_subset), UK_bing_combined_subset))
  
  colnames(UK_bing_combined_subset) <- c("Group", "Period1","Period2","Period3")
  
  UK_bing_combined_subset$Period1 <- as.numeric(UK_bing_combined_subset$Period1)
  UK_bing_combined_subset$Period2 <- as.numeric(UK_bing_combined_subset$Period2)
  UK_bing_combined_subset$Period3 <- as.numeric(UK_bing_combined_subset$Period3)
  
  
  df.UK_bing_combined_subset_2 <- melt(UK_bing_combined_subset, 
                                       id.vars = c("Group"), 
                                       measure.vars = colnames(UK_bing_combined_subset)[2:length(colnames(UK_bing_combined_subset))],
                                       variable.name = "Key",
                                       value.name = "Score")
  
  col = c(viridis(10)[9], viridis(10)[6], viridis(10)[3])
  linetp <- c(rep(1, nrow(df.UK_bing_combined_subset_2)/3),
              rep(2, nrow(df.UK_bing_combined_subset_2)/3), 
              rep(3, nrow(df.UK_bing_combined_subset_2)/3))
  
  if(regions[k]=="North West"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    North_West = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  North_West
  
  if(regions[k]=="North East"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    North_East = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Yorkshire and the Humber"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Yorkshire_and_the_Humber = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="West Midlands"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    West_Midlands = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar() 
  }
  
  max(df.UK_bing_combined_subset_2$Score)  
  
  if(regions[k]=="East Midlands"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    East_Midlands = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Eastern"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Eastern = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="Wales"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    Wales = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="South West"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    South_West = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  if(regions[k]=="South East"){
    df.UK_bing_combined_subset_2 <- df.UK_bing_combined_subset_2 %>%
      #group_by(Key)%>%
      arrange(desc(Key), Group)%>%
      as.data.frame()
    
    South_East = df.UK_bing_combined_subset_2 %>%
      ggplot(aes(x = Group,y = Score,color = Key,group = Key,fill = Key)) +
      geom_polygon(size = 1, alpha = .1, fill=NA,linetype=linetp) +
      scale_color_manual(values=col)+
      geom_point(size = 3) +
      ylim(min(df.UK_bing_combined_subset_2$Score)-5, max(df.UK_bing_combined_subset_2$Score)) + ggtitle(regions[k])  + 
      scale_x_discrete() +
      xlab("")+
      theme_light() +
      #scale_color_manual(values = c("Red", "Blue")) +
      #scale_fill_manual(values = c("Red", "Blue")) +
      coord_radar() #+
    #theme_radar()
  }
  
  #mapping, plot maps for the web-app
  #https://gis.stackexchange.com/questions/203581/r-ggplot2-adding-an-additional-line-polygon-to-a-choropleth-map 
  #Sys.sleep(2)
  #two-tailed 
  #https://www.investopedia.com/terms/t/two-tailed-test.asp
  
  #dev.off()
  
}


dev.new()

ggarrange(
  North_West, North_East, Yorkshire_and_the_Humber, West_Midlands,
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)

#Add figure

ggarrange(
  East_Midlands,            
  Eastern, Wales, South_West, 
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)

#Add figure

ggarrange(
  South_East,
  common.legend = TRUE, nrow=2, ncol=2, legend = "bottom"
)

Add figure


```


(ii) _Likert Chart_ - To show the proportion of tweet types and sentiments per PFA
(iii) _Spatial Maps_ - To show the spatial distribution of significant testing results

First



